"""
Unified Backtesting Module

This module provides unified backtesting functionality that:
1. Uses method-specific parameter optimization (once per product)
2. Groups products by forecasting method
3. Works at daily level for forecasting
4. Aggregates results at risk_period level
5. Supports multiple forecasting methods with extensible framework
6. Supports multiple forecast methods per product-location

PERFORMANCE OPTIMIZATIONS (3-5x performance improvement):
7. Intelligent processing method selection:
   - >1000 tasks: Vectorized processing (all dates per product)
   - <1000 tasks: Parallel processing (individual tasks)
   - Sequential processing (single threaded)
8. Comprehensive data caching system:
   - Pre-computed product data filters
   - Cached aggregated demand calculations
   - Eliminated redundant filtering operations
9. Vectorized backtesting architecture:
   - Traditional: Process 1 forecast at a time (Product A, Date 1 â†’ Product A, Date 2 â†’ Product B, Date 1...)
   - Optimized: Process all dates for 1 product at a time (Product A: [Date 1, Date 2, Date 3...] â†’ Product B: [Date 1, Date 2, Date 3...])
10. Batch operations for visualization data generation:
    - Converted from row-by-row to pandas vectorized operations
    - Pre-aggregated forecast data to avoid repeated filtering
    - Used cached product data for historical calculations
"""

import pandas as pd
import numpy as np
from datetime import date, timedelta, datetime
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import logging
from concurrent.futures import ProcessPoolExecutor, as_completed
import time
from tqdm import tqdm
import json

from .config import BacktestConfig
from data.loader import DataLoader
from ..validation.demand_validator import DemandValidator
from ..outlier.handler import OutlierHandler
from ..data.aggregator import DemandAggregator
from ..forecasting.parameter_optimizer import ParameterOptimizerFactory
from ..forecasting.core_engine import CoreForecastingEngine
from ..forecasting.base import calculate_forecast_metrics
from ..utils.logger import get_logger, configure_workflow_logging
from ..validation.product_master_schema import ProductMasterSchema


class UnifiedBacktester:
    """
    Unified backtesting class that supports multiple forecasting methods
    with method-specific parameter optimization (once per product).
    """

    def __init__(self, config: BacktestConfig):
        """Initialize the unified backtester with configuration."""
        self.config = config
        
        # Setup logging for this backtesting run
        self.logger = configure_workflow_logging(
            workflow_name="unified_backtesting",
            log_level=config.log_level,
            log_dir="output/logs"
        )

        # Data storage
        self.demand_data: Optional[pd.DataFrame] = None
        self.product_master_data: Optional[pd.DataFrame] = None
        self.expanded_product_master_data: Optional[pd.DataFrame] = (
            None  # Expanded for methods
        )
        self.outlier_data: Optional[pd.DataFrame] = None

        # Results storage
        self.backtest_results: List[Dict[str, Any]] = []
        self.accuracy_metrics: List[Dict[str, Any]] = []
        self.forecast_comparisons: List[Dict[str, Any]] = []

        # Product grouping by forecasting method
        self.product_groups = {}

        # Core forecasting engine
        self.forecasting_engine = CoreForecastingEngine()

        # Optimized parameters cache (set once per product)
        self.optimized_parameters_cache = {}
        
        # OPTIMIZATION: Data caching system for performance
        self.product_data_cache = {}  # Cache filtered product data
        self.aggregated_demand_cache = {}  # Cache aggregated demands

        # Validate configuration
        if not self.config.validate_dates():
            raise ValueError(
                "Invalid date configuration: analysis_start_date <= analysis_end_date"
            )

    def run(self) -> Dict:
        """
        Run the complete unified backtesting process.

        Returns:
            Dictionary with backtesting summary and results
        """
        start_time = time.time()
        self.logger.info("Starting unified backtesting process")
        self.logger.info(f"Configuration: {self.config.__dict__}")

        self.logger.info("ðŸš€ UNIFIED BACKTESTING WORKFLOW")

        try:
            # Step 1: Load and validate data
            step_start = time.time()
            self.logger.log_workflow_step("Loading and validating data", 1, 7)
            self._load_data()
            self.logger.log_step_completion("Data loading", time.time() - step_start)

            # Step 1.5: Expand product master for multiple methods
            step_start = time.time()
            self.logger.log_workflow_step("Expanding product master for multiple methods", 2, 7)
            self._expand_product_master_for_methods()
            self.logger.log_step_completion("Product master expansion", time.time() - step_start)

            # Print initial summary
            self._print_initial_summary()

            # Step 2: Group products by forecasting method
            step_start = time.time()
            self.logger.log_workflow_step("Grouping products by forecasting method", 3, 7)
            self._group_products_by_method()
            self.logger.log_step_completion("Product grouping", time.time() - step_start)

            # Step 3: Handle outliers across the entire analysis period
            step_start = time.time()
            self.logger.log_workflow_step("Handling outliers", 4, 7)
            self._handle_outliers()
            self.logger.log_step_completion("Outlier handling", time.time() - step_start)

            # Step 4: Optimize parameters once for each product-location-method
            step_start = time.time()
            self.logger.log_workflow_step("Optimizing parameters", 5, 7)
            self._optimize_parameters_once()
            self.logger.log_step_completion("Parameter optimization", time.time() - step_start)

            # Step 5: Run backtesting for each analysis date
            step_start = time.time()
            self.logger.log_workflow_step("Running backtesting", 6, 7)
            self._run_unified_backtesting()
            self.logger.log_step_completion("Backtesting", time.time() - step_start)

            # Step 6: Calculate accuracy metrics
            step_start = time.time()
            self.logger.log_workflow_step("Calculating accuracy metrics", 7, 7)
            self._calculate_accuracy_metrics()
            self.logger.log_step_completion("Accuracy metrics", time.time() - step_start)

            # Step 7: Save results
            step_start = time.time()
            self.logger.log_workflow_step("Saving results", 7, 7)
            self._save_results()
            self.logger.log_step_completion("Saving results", time.time() - step_start)

            # Generate summary
            total_time = time.time() - start_time
            summary = self._generate_summary(total_time)

            self.logger.info("Unified backtesting completed successfully")
            return summary

        except Exception as e:
            self.logger.error(f"Unified backtesting failed: {e}", exc_info=True)
            raise

    def _load_data(self):
        """Load and validate demand and product master data using the new DataLoader."""
        self.logger.info("Step 1: Loading and validating data with DataLoader")

        # Initialize DataLoader
        # The config_path will be derived from self.config.data_dir if needed,
        # assuming a standard structure. For now, we rely on the default path.
        loader = DataLoader()

        # Load product master data first
        self.product_master_data = loader.load_product_master()

        # Validate product master schema
        ProductMasterSchema.validate_dataframe(self.product_master_data)
        self.product_master_data = ProductMasterSchema.standardize_dataframe(
            self.product_master_data
        )

        # Load demand data filtered by product master
        self.demand_data = loader.load_outflow(
            product_master=self.product_master_data
        )

        self.logger.info(f"Loaded {len(self.demand_data)} demand records (filtered by product master)")
        self.logger.info(
            f"Loaded {len(self.product_master_data)} product master records"
        )

    def _expand_product_master_for_methods(self):
        """Expand product master to create separate rows for each forecast method."""
        self.logger.info("Step 1.5: Expanding product master for multiple methods")

        self.expanded_product_master_data = (
            ProductMasterSchema.expand_product_master_for_methods(
                self.product_master_data
            )
        )

        self.logger.info(
            f"Expanded to {len(self.expanded_product_master_data)} product-method combinations"
        )

        # Log method distribution
        method_counts = self.expanded_product_master_data[
            "forecast_method"
        ].value_counts()
        for method, count in method_counts.items():
            self.logger.info(
                f"Method '{method}': {count} product-location combinations"
            )

    def _print_initial_summary(self):
        """Print initial summary of the backtesting configuration and data."""
        self.logger.info("UNIFIED BACKTESTING SUMMARY")
        self.logger.info("Data Configuration:")
        self.logger.info(f"  â€¢ Total demand records: {len(self.demand_data):,}")
        self.logger.info(f"  â€¢ Original product-location combinations: {len(self.product_master_data):,}")
        self.logger.info(f"  â€¢ Expanded product-method combinations: {len(self.expanded_product_master_data):,}")

        self.logger.info("Date Configuration:")
        self.logger.info(f"  â€¢ Analysis period: {self.config.analysis_start_date} to {self.config.analysis_end_date}")
        self.logger.info(f"  â€¢ Analysis dates: {len(self.config.get_analysis_dates())} dates")

        self.logger.info("Forecasting Configuration:")
        self.logger.info(f"  â€¢ Default horizon: {self.config.default_horizon} risk periods")
        self.logger.info(f"  â€¢ Demand frequency: {self.config.demand_frequency}")
        self.logger.info(f"  â€¢ Aggregation: {'Enabled' if self.config.aggregation_enabled else 'Disabled'}")

        self.logger.info("Processing Configuration:")
        self.logger.info(f"  â€¢ Batch size: {self.config.batch_size}")
        self.logger.info(f"  â€¢ Max workers: {self.config.max_workers}")
        self.logger.info(f"  â€¢ Expected total forecasts: {len(self.config.get_analysis_dates()) * len(self.expanded_product_master_data):,}")

    def _group_products_by_method(self):
        """Group products by their forecasting method for efficient processing."""
        self.logger.info("Step 2: Grouping products by forecasting method")

        # Group by forecasting method using expanded product master
        for _, product_record in self.expanded_product_master_data.iterrows():
            method = product_record["forecast_method"]

            if method not in self.product_groups:
                self.product_groups[method] = []

            self.product_groups[method].append(
                {
                    "product_id": product_record["product_id"],
                    "location_id": product_record["location_id"],
                    "forecast_method": method,
                    "product_record": product_record,
                }
            )

        # Log grouping results
        for method, products in self.product_groups.items():
            self.logger.info(
                f"Method '{method}': {len(products)} product-method combinations"
            )

    def _handle_outliers(self):
        """Handle outliers across the entire analysis period."""
        self.logger.info("Step 3: Handling outliers")

        # Create outlier handler
        outlier_handler = OutlierHandler()

        # Process outliers using the actual loaded data
        result = outlier_handler.process_demand_outliers_with_data(
            self.demand_data,
            self.product_master_data,
            default_method="iqr",
            default_threshold=1.5,
        )

        # Use cleaned data as the outlier-processed data
        self.outlier_data = result["cleaned_data"]
        self.logger.info(
            f"Outlier processing completed: {len(self.outlier_data)} cleaned records"
        )

    def _optimize_parameters_once(self):
        """Step 4: Optimize parameters once for each product-location-method."""
        self.logger.info("Step 4: Optimizing parameters once for each product-location-method")

        optimized_count = 0
        failed_count = 0
        total_products = sum(len(products) for products in self.product_groups.values())

        self.logger.info(f"ðŸ“Š Total products to optimize: {total_products}")

        for method_name, products in self.product_groups.items():
            self.logger.info(f"Optimizing parameters for {len(products)} products with method: {method_name}")
            self.logger.info(f"ðŸ“ˆ Method: {method_name} ({len(products)} products)")

            # Progress bar for each method
            for product_info in tqdm(
                products, desc=f"Optimizing {method_name}", unit="product"
            ):
                product_record = product_info["product_record"]
                product_id = product_record["product_id"]
                location_id = product_record["location_id"]
                forecast_method = product_record["forecast_method"]

                # Get all available data for this product-location
                all_data = self._get_all_data_for_product_location(
                    product_id, location_id
                )

                if len(all_data) == 0:
                    self.logger.warning(
                        f"No data found for {product_id}-{location_id}, skipping parameter optimization"
                    )
                    failed_count += 1
                    continue

                # Get base parameters from product master
                base_parameters = {
                    "window_length": product_record.get("forecast_window_length")
                    * product_record.get("risk_period", 1),
                    "horizon": product_record.get(
                        "forecast_horizon", self.config.default_horizon
                    ),
                    "risk_period_days": product_record.get("risk_period"),
                }

                # Optimize parameters using all available data
                try:
                    optimizer = ParameterOptimizerFactory.get_optimizer(forecast_method)
                    
                    # Add log_level to base_parameters so it gets passed to the forecaster
                    base_parameters["log_level"] = self.config.log_level
                    
                    optimized_parameters = optimizer.optimize_parameters(
                        all_data, product_record, base_parameters
                    )

                    # Cache the optimized parameters with method included
                    product_key = f"{product_id}_{location_id}_{forecast_method}"
                    self.optimized_parameters_cache[product_key] = optimized_parameters

                    optimized_count += 1
                    self.logger.debug(
                        f"Optimized parameters for {product_id}-{location_id}-{forecast_method}: {optimized_parameters}"
                    )

                except Exception as e:
                    self.logger.error(
                        f"Error optimizing parameters for {product_id}-{location_id}-{forecast_method}: {e}"
                    )
                    failed_count += 1
                    continue

        self.logger.info(f"âœ… Parameter optimization completed!")
        self.logger.info(f"ðŸ“Š Successfully optimized: {optimized_count}/{total_products} products")
        self.logger.info(f"âŒ Failed optimizations: {failed_count} products")
        self.logger.info(
            f"Parameter optimization completed for {len(self.optimized_parameters_cache)} product-location-method combinations"
        )

    def _get_all_data_for_product_location(
        self, product_id: str, location_id: str
    ) -> pd.DataFrame:
        """Get all available data for a product-location combination."""
        # Use outlier data if available, otherwise use original demand data
        input_data = (
            self.outlier_data if self.outlier_data is not None else self.demand_data
        )

        # Filter for this product-location
        product_data = input_data[
            (input_data["product_id"] == product_id)
            & (input_data["location_id"] == location_id)
        ]

        return product_data

    def _run_unified_backtesting(self):
        """Step 5: Run backtesting for each analysis date."""
        self.logger.info("Step 5: Running unified backtesting")

        # Get analysis dates
        analysis_dates = self.config.get_analysis_dates()

        # Create product-location-method combinations for processing
        product_locations = []
        for method_name, products in self.product_groups.items():
            for product_info in products:
                product_locations.append(product_info)

        total_tasks = len(analysis_dates) * len(product_locations)
        self.logger.info(
            f"Running backtesting for {len(analysis_dates)} dates and {len(product_locations)} product-method combinations"
        )

        # OPTIMIZATION: Intelligent processing method selection based on task volume
        if total_tasks > 1000:
            # Use vectorized processing for large workloads (process all dates per product)
            mode = "Vectorized"
            self.logger.info(f"ðŸš€ Using {mode} backtesting for {total_tasks:,} tasks (>1000 threshold)")
            self._run_unified_backtesting_vectorized(analysis_dates, product_locations)
        elif self.config.max_workers > 1:
            # Use parallel processing for smaller workloads
            mode = "Parallel"
            workers_info = f" ({self.config.max_workers} workers)"
            self.logger.info(f"ðŸš€ Using {mode} backtesting{workers_info}: {total_tasks:,} tasks")
            self._run_unified_backtesting_parallel(analysis_dates, product_locations)
        else:
            # Use sequential processing
            mode = "Sequential"
            self.logger.info(f"ðŸš€ Using {mode} backtesting: {total_tasks:,} tasks")
            self._run_unified_backtesting_sequential(analysis_dates, product_locations)

    def _run_unified_backtesting_sequential(
        self, analysis_dates: List[date], product_locations: List[Dict]
    ):
        """Run backtesting sequentially."""
        self.logger.info("Running backtesting sequentially")

        total_tasks = len(analysis_dates) * len(product_locations)
        completed_count = 0
        successful_count = 0

        for analysis_date in tqdm(analysis_dates, desc="Analysis dates", unit="date"):
            cutoff_date = self._get_cutoff_date(analysis_date)

            for product_info in product_locations:
                result = self._run_unified_forecast_for_product_location(
                    analysis_date,
                    cutoff_date,
                    product_info["product_id"],
                    product_info["location_id"],
                    product_info["product_record"],
                )

                completed_count += 1
                if result:
                    successful_count += 1
                    self.backtest_results.append(result)

        self.logger.info(f"âœ… Sequential backtesting completed!")
        self.logger.info(f"ðŸ“Š Successfully processed: {successful_count:,}/{total_tasks:,} tasks")
        self.logger.info(f"ðŸ“ˆ Generated {len(self.backtest_results):,} forecast results")

    def _run_unified_backtesting_parallel(
        self, analysis_dates: List[date], product_locations: List[Dict]
    ):
        """Run backtesting in parallel using the new DataLoader with preloading."""
        self.logger.info(
            f"Running backtesting in parallel with {self.config.max_workers} workers"
        )

        # Step 1: Preload data in the main process
        self.logger.info("Preloading data for parallel processing...")
        preloaded_data = DataLoader.preload_for_parallel_processing()
        self.logger.info("Data preloading complete.")

        # Step 2: Create tasks with a reference to the preloaded data
        tasks = []
        for analysis_date in analysis_dates:
            cutoff_date = self._get_cutoff_date(analysis_date)

            for product_info in product_locations:
                task = {
                    "analysis_date": analysis_date,
                    "cutoff_date": cutoff_date,
                    "product_id": product_info["product_id"],
                    "location_id": product_info["location_id"],
                    "product_record": product_info["product_record"],
                    "preloaded_data": preloaded_data,  # Pass preloaded data
                }
                tasks.append(task)

        # Step 3: Process tasks in parallel
        with ProcessPoolExecutor(max_workers=self.config.max_workers) as executor:
            futures = [
                executor.submit(self._process_single_backtest_task, task)
                for task in tasks
            ]

            completed_count = 0
            successful_count = 0

            for future in tqdm(
                as_completed(futures),
                total=len(futures),
                desc="Processing forecast tasks",
                unit="task",
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]",
            ):
                completed_count += 1
                result = future.result()
                if result:
                    successful_count += 1
                    if "comparisons" in result:
                        comparisons = result.pop("comparisons", [])
                        if comparisons:
                            self.forecast_comparisons.extend(comparisons)
                    self.backtest_results.append(result)

        self.logger.info(f"âœ… Parallel backtesting completed!")
        self.logger.info(f"ðŸ“Š Successfully processed: {successful_count:,}/{len(tasks):,} tasks")
        self.logger.info(f"ðŸ“ˆ Generated {len(self.backtest_results):,} forecast results")
        self.logger.info(f"ðŸ“‹ Generated {len(self.forecast_comparisons):,} forecast comparisons")

    def _run_unified_backtesting_vectorized(
        self, analysis_dates: List[date], product_locations: List[Dict]
    ):
        """Run backtesting using vectorized approach - process all dates for one product at a time."""
        self.logger.info(
            f"Running vectorized backtesting with {self.config.max_workers} workers"
        )

        # OPTIMIZATION: Pre-compute and cache product data to eliminate redundant filtering
        self.logger.info("Pre-computing product data cache...")
        self._precompute_product_data_cache(product_locations)
        self.logger.info("Product data cache complete.")

        # OPTIMIZATION: Create tasks by product-location-method (not by individual dates)
        # Each task processes ALL analysis dates for one product
        product_tasks = []
        for product_info in product_locations:
            task = {
                "product_id": product_info["product_id"],
                "location_id": product_info["location_id"],
                "product_record": product_info["product_record"],
                "analysis_dates": analysis_dates,  # All dates for this product
            }
            product_tasks.append(task)

        total_products = len(product_tasks)
        total_forecasts = len(analysis_dates) * len(product_locations)
        self.logger.info(f"ðŸš€ Processing {total_products:,} product tasks (covering {total_forecasts:,} forecasts)")

        # Process vectorized tasks in parallel
        with ProcessPoolExecutor(max_workers=self.config.max_workers) as executor:
            futures = [
                executor.submit(self._process_product_all_dates, task)
                for task in product_tasks
            ]

            completed_count = 0
            successful_forecasts = 0

            for future in tqdm(
                as_completed(futures),
                total=len(futures),
                desc="Processing product batches",
                unit="product",
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]",
            ):
                completed_count += 1
                results = future.result()
                if results:
                    successful_forecasts += len(results)
                    for result in results:
                        if result:
                            if "comparisons" in result:
                                comparisons = result.pop("comparisons", [])
                                if comparisons:
                                    self.forecast_comparisons.extend(comparisons)
                            self.backtest_results.append(result)

        self.logger.info(f"âœ… Vectorized backtesting completed!")
        self.logger.info(f"ðŸ“Š Successfully processed: {successful_forecasts:,}/{total_forecasts:,} forecasts")
        self.logger.info(f"ðŸ“ˆ Generated {len(self.backtest_results):,} forecast results")
        self.logger.info(f"ðŸ“‹ Generated {len(self.forecast_comparisons):,} forecast comparisons")

    def _precompute_product_data_cache(self, product_locations: List[Dict]):
        """Pre-compute and cache product data to eliminate redundant filtering operations."""
        # Use outlier data if available, otherwise use original demand data
        input_data = (
            self.outlier_data if self.outlier_data is not None else self.demand_data
        )
        
        # Group by product-location to avoid duplicate caching
        product_location_keys = set()
        for product_info in product_locations:
            product_id = product_info["product_id"]
            location_id = product_info["location_id"]
            key = f"{product_id}_{location_id}"
            product_location_keys.add((key, product_id, location_id))
        
        # Pre-filter and cache data for each unique product-location
        for key, product_id, location_id in tqdm(
            product_location_keys, desc="Caching product data", unit="product"
        ):
            # Filter data for this product-location combination
            product_data = input_data[
                (input_data["product_id"] == product_id)
                & (input_data["location_id"] == location_id)
            ].copy()
            
            # Sort by date for efficient date-based filtering later
            product_data = product_data.sort_values("date")
            
            # Cache the filtered data
            self.product_data_cache[key] = product_data

    def _process_product_all_dates(self, task: Dict) -> List[Optional[Dict[str, Any]]]:
        """
        Process all analysis dates for one product-location-method combination.
        This is the core of the vectorized approach - reduces task overhead and improves data locality.
        """
        product_id = task["product_id"]
        location_id = task["location_id"]
        product_record = task["product_record"]
        analysis_dates = task["analysis_dates"]
        
        # Initialize worker logger
        from ..utils.logger import get_logger
        logger = get_logger(__name__)
        
        try:
            # Initialize DataLoader for the worker
            loader = DataLoader()
            
            # Get cached product data or filter if not cached
            product_key = f"{product_id}_{location_id}"
            if hasattr(self, 'product_data_cache') and product_key in self.product_data_cache:
                # Use cached data (main process)
                product_data = self.product_data_cache[product_key]
            else:
                # Worker process - filter data directly  
                demand_data = loader.load_outflow(product_master=loader.load_product_master())
                product_data = demand_data[
                    (demand_data["product_id"] == product_id)
                    & (demand_data["location_id"] == location_id)
                ].sort_values("date")

            if len(product_data) == 0:
                logger.warning(f"No data found for {product_id}-{location_id}")
                return []

            # Get forecast method and optimized parameters
            forecast_method = product_record["forecast_method"]
            product_method_key = f"{product_id}_{location_id}_{forecast_method}"
            optimized_parameters = self.optimized_parameters_cache.get(product_method_key, {})

            # Process all analysis dates for this product
            results = []
            for analysis_date in analysis_dates:
                try:
                    # Get cutoff date and filter data up to that point
                    cutoff_date = self._get_cutoff_date(analysis_date)
                    daily_data = product_data[product_data["date"] <= cutoff_date]

                    if len(daily_data) == 0:
                        continue

                    # Generate forecast using the core forecasting engine
                    horizon = product_record.get("forecast_horizon", self.config.default_horizon)
                    forecast_result = self.forecasting_engine.generate_forecast(
                        daily_data, product_record, analysis_date, horizon, optimized_parameters
                    )

                    if not forecast_result or "forecast_values" not in forecast_result:
                        continue

                    # Get actual aggregated demands for comparison
                    actual_demands = self._get_actual_aggregated_demand_cached(
                        analysis_date, product_id, location_id, product_record, product_data
                    )

                    if not actual_demands:
                        continue

                    # Calculate forecast metrics
                    forecast_values = forecast_result["forecast_values"]
                    metrics = calculate_forecast_metrics(actual_demands, forecast_values)

                    # Create backtest result
                    result = {
                        "analysis_date": analysis_date,
                        "product_id": product_id,
                        "location_id": location_id,
                        "forecast_method": forecast_method,
                        "risk_period": product_record.get("risk_period"),
                        "demand_frequency": product_record.get("demand_frequency"),
                        "forecast_values": forecast_values,
                        "actual_demands": actual_demands,
                        "metrics": metrics,
                        "parameters": optimized_parameters,
                        "first_date_used": forecast_result.get("first_date_used"),
                    }

                    # Generate forecast comparisons for visualization
                    if not isinstance(forecast_values, list):
                        forecast_values = [forecast_values]

                    comparisons = self._generate_forecast_comparisons(
                        analysis_date,
                        product_id,
                        location_id,
                        forecast_method,
                        actual_demands,
                        forecast_values,
                        product_record.get("risk_period"),
                        product_record.get("demand_frequency"),
                        forecast_result.get("first_date_used"),
                    )

                    result["comparisons"] = comparisons
                    results.append(result)

                except Exception as e:
                    logger.error(f"Error processing {product_id}-{location_id} for {analysis_date}: {e}")
                    continue

            return results

        except Exception as e:
            logger.error(f"Error in vectorized processing for {product_id}-{location_id}: {e}")
            return []

    def _process_single_backtest_task(self, task: Dict) -> Optional[Dict[str, Any]]:
        """Process a single backtest task using preloaded data."""
        # The preloaded data is passed in the task dictionary.
        # The DataLoader will be implicitly created for the worker process.
        return self._run_unified_forecast_for_product_location(
            task["analysis_date"],
            task["cutoff_date"],
            task["product_id"],
            task["location_id"],
            task["product_record"],
            task.get("preloaded_data")  # Pass preloaded data to the forecast function
        )

    def _run_unified_forecast_for_product_location(
        self,
        analysis_date: date,
        cutoff_date: date,
        product_id: str,
        location_id: str,
        product_record: pd.Series,
        preloaded_data: Optional[Dict[str, pd.DataFrame]] = None
    ) -> Optional[Dict[str, Any]]:
        """Run unified forecast for a single product-location-method combination."""
        try:
            # Step 1: Initialize DataLoader for the worker
            # If preloaded_data is provided, it creates a worker-optimized loader.
            # Otherwise, it falls back to a standard loader (for sequential mode).
            if preloaded_data:
                loader = DataLoader.create_for_worker(preloaded_data)
                # Use preloaded data directly for efficiency
                self.demand_data = loader.load_outflow(product_master=loader.load_product_master())
            else:
                loader = DataLoader()
                # Ensure data is loaded if not already present (sequential case)
                if self.demand_data is None:
                    self._load_data()


            # Get daily data for this product-location up to cutoff date
            daily_data = self._get_daily_data_for_product_location(
                product_id, location_id, cutoff_date
            )

            if len(daily_data) == 0:
                self.logger.warning(
                    f"No data found for {product_id}-{location_id} up to {cutoff_date}"
                )
                return None

            # Get forecast method from product record
            forecast_method = product_record["forecast_method"]

            # Get optimized parameters for this product-location-method
            product_key = f"{product_id}_{location_id}_{forecast_method}"
            optimized_parameters = self.optimized_parameters_cache.get(product_key, {})

            # Generate forecast using the core forecasting engine
            horizon = product_record.get(
                "forecast_horizon", self.config.default_horizon
            )
            forecast_result = self.forecasting_engine.generate_forecast(
                daily_data, product_record, analysis_date, horizon, optimized_parameters
            )

            if not forecast_result or "forecast_values" not in forecast_result:
                self.logger.warning(
                    f"No forecast generated for {product_id}-{location_id}-{forecast_method}"
                )
                return None

            # Get actual aggregated demands for comparison
            actual_demands = self._get_actual_aggregated_demand(
                analysis_date, product_id, location_id, product_record
            )

            if not actual_demands:
                self.logger.warning(
                    f"No actual demands found for {product_id}-{location_id} on {analysis_date}"
                )
                return None

            # Calculate forecast metrics
            forecast_values = forecast_result["forecast_values"]
            metrics = calculate_forecast_metrics(actual_demands, forecast_values)

            # Create backtest result
            result = {
                "analysis_date": analysis_date,
                "product_id": product_id,
                "location_id": location_id,
                "forecast_method": forecast_method,
                "risk_period": product_record.get("risk_period"),
                "demand_frequency": product_record.get("demand_frequency"),
                "forecast_values": forecast_values,
                "actual_demands": actual_demands,
                "metrics": metrics,
                "parameters": optimized_parameters,
                "first_date_used": forecast_result.get("first_date_used"),
            }

            # Generate forecast comparisons for visualization
            if not isinstance(forecast_values, list):
                forecast_values = [forecast_values]

            comparisons = self._generate_forecast_comparisons(
                analysis_date,
                product_id,
                location_id,
                forecast_method,
                actual_demands,
                forecast_values,
                product_record.get("risk_period"),
                product_record.get("demand_frequency"),
                forecast_result.get("first_date_used"),
            )

            result["comparisons"] = comparisons

            return result

        except Exception as e:
            # It's better to log the forecast_method if it's available
            method_info = product_record.get("forecast_method", "unknown_method")
            self.logger.error(
                f"Error in backtest for {product_id}-{location_id}-{method_info}: {e}",
                exc_info=True # Include stack trace for better debugging
            )
            return None

    def _get_daily_data_for_product_location(
        self, product_id: str, location_id: str, cutoff_date: date
    ) -> pd.DataFrame:
        """Get daily data for a product-location up to cutoff date."""
        # Use outlier data if available, otherwise use original demand data
        input_data = (
            self.outlier_data if self.outlier_data is not None else self.demand_data
        )

        # Filter for this product-location up to cutoff date
        product_data = input_data[
            (input_data["product_id"] == product_id)
            & (input_data["location_id"] == location_id)
            & (input_data["date"] <= cutoff_date)
        ]

        return product_data.sort_values("date")

    def _get_actual_aggregated_demand(
        self,
        analysis_date: date,
        product_id: str,
        location_id: str,
        product_record: pd.Series,
    ) -> Optional[List[float]]:
        """Get actual aggregated demands for comparison."""
        try:
            # Use outlier data if available, otherwise use original demand data
            input_data = (
                self.outlier_data if self.outlier_data is not None else self.demand_data
            )

            # Get parameters
            risk_period = product_record.get("risk_period")
            demand_frequency = product_record.get("demand_frequency")
            horizon = product_record.get(
                "forecast_horizon", self.config.default_horizon
            )

            # Calculate risk period in days
            if demand_frequency == "d":
                risk_period_days = risk_period
            elif demand_frequency == "w":
                risk_period_days = risk_period * 7
            elif demand_frequency == "m":
                risk_period_days = risk_period * 30
            else:
                raise ValueError(f"Invalid demand frequency: {demand_frequency}")

            # Get actual demands for each risk period
            actual_demands = []
            for i in range(horizon):
                period_start = analysis_date + timedelta(days=i * risk_period_days)
                period_end = analysis_date + timedelta(days=(i + 1) * risk_period_days)

                # Filter data for this period
                period_data = input_data[
                    (input_data["product_id"] == product_id)
                    & (input_data["location_id"] == location_id)
                    & (input_data["date"] >= period_start)
                    & (input_data["date"] < period_end)
                ]

                # Aggregate demand for this period
                if len(period_data) > 0:
                    period_demand = period_data["demand"].sum()
                else:
                    period_demand = 0.0

                actual_demands.append(period_demand)

            return actual_demands

        except Exception as e:
            self.logger.error(
                f"Error getting actual demands for {product_id}-{location_id}: {e}"
            )
            return None

    def _get_actual_aggregated_demand_cached(
        self,
        analysis_date: date,
        product_id: str,
        location_id: str,
        product_record: pd.Series,
        product_data: pd.DataFrame,
    ) -> Optional[List[float]]:
        """
        Get actual aggregated demands for comparison using cached product data.
        This eliminates redundant filtering operations.
        """
        try:
            # Get parameters
            risk_period = product_record.get("risk_period")
            demand_frequency = product_record.get("demand_frequency")
            horizon = product_record.get("forecast_horizon", self.config.default_horizon)

            # Calculate risk period in days
            if demand_frequency == "d":
                risk_period_days = risk_period
            elif demand_frequency == "w":
                risk_period_days = risk_period * 7
            elif demand_frequency == "m":
                risk_period_days = risk_period * 30
            else:
                raise ValueError(f"Invalid demand frequency: {demand_frequency}")

            # Get actual demands for each risk period using cached data
            actual_demands = []
            for i in range(horizon):
                period_start = analysis_date + timedelta(days=i * risk_period_days)
                period_end = analysis_date + timedelta(days=(i + 1) * risk_period_days)

                # Filter cached data for this period - much faster than filtering full dataset
                period_data = product_data[
                    (product_data["date"] >= period_start)
                    & (product_data["date"] < period_end)
                ]

                # Aggregate demand for this period
                period_demand = period_data["demand"].sum() if len(period_data) > 0 else 0.0
                actual_demands.append(period_demand)

            return actual_demands

        except Exception as e:
            from ..utils.logger import get_logger
            logger = get_logger(__name__)
            logger.error(f"Error getting cached actual demands for {product_id}-{location_id}: {e}")
            return None

    def _get_cutoff_date(self, analysis_date: date) -> date:
        """Get cutoff date for forecasting (exclusive)."""
        return analysis_date

    def _calculate_accuracy_metrics(self):
        """Step 6: Calculate accuracy metrics."""
        self.logger.info("Step 6: Calculating accuracy metrics")

        for result in self.backtest_results:
            metrics = result["metrics"]

            accuracy_record = {
                "analysis_date": result["analysis_date"],
                "product_id": result["product_id"],
                "location_id": result["location_id"],
                "forecast_method": result["forecast_method"],  # Include method
                "mae": metrics.get("mae", 0),
                "mape": metrics.get("mape", 0),
                "rmse": metrics.get("rmse", 0),
                "bias": metrics.get("bias", 0),
                "demand_frequency": result["demand_frequency"],
            }

            self.accuracy_metrics.append(accuracy_record)

        self.logger.info(
            f"Calculated accuracy metrics for {len(self.accuracy_metrics)} forecasts"
        )

    def _generate_forecast_comparisons(
        self,
        analysis_date: date,
        product_id: str,
        location_id: str,
        forecast_method: str,
        actual_demands: List[float],
        forecast_values: List[float],
        risk_period: int,
        demand_frequency: str,
        first_date_used: Optional[date] = None,
    ):
        """Generate forecast comparison data for visualization."""
        self.logger.debug(
            f"Generating comparisons for {product_id}-{location_id}-{forecast_method}: {len(actual_demands)} actual, {len(forecast_values)} forecast"
        )

        # Convert numpy arrays to regular Python types
        actual_demands = [float(actual) for actual in actual_demands]
        forecast_values = [float(forecast) for forecast in forecast_values]

        # Create comparison records
        comparisons = []
        for i, (actual, forecast) in enumerate(zip(actual_demands, forecast_values)):
            # Calculate risk period start and end to avoid overlap
            if i == 0:
                # First step: start on analysis date
                risk_period_start = analysis_date
            else:
                # Subsequent steps: start the day after the previous period ends
                risk_period_start = (
                    analysis_date + timedelta(days=i * risk_period) + timedelta(days=1)
                )

            risk_period_end = analysis_date + timedelta(days=(i + 1) * risk_period)

            comparison = {
                "analysis_date": analysis_date,
                "product_id": product_id,
                "location_id": location_id,
                "forecast_method": forecast_method,  # Include method in comparisons
                "risk_period": risk_period,  # Use the actual risk_period from product master
                "step": i + 1,  # Add 'step' column for simulation compatibility
                "risk_period_start": risk_period_start,
                "risk_period_end": risk_period_end,
                "actual_demand": actual,
                "forecast_demand": forecast,
                "forecast_error": actual - forecast,
                "absolute_error": abs(actual - forecast),
                "percentage_error": (
                    ((actual - forecast) / actual * 100) if actual != 0 else 0
                ),
                "demand_frequency": demand_frequency,
                "first_date_used": first_date_used,  # Include first date of data used
            }

            comparisons.append(comparison)

        return comparisons

    def _save_results(self):
        """Save backtesting results."""
        self.logger.info("Step 7: Saving results")

        # Initialize DataLoader
        from data.loader import DataLoader
        loader = DataLoader()

        # Save backtest results
        if self.backtest_results:
            results_df = pd.DataFrame(self.backtest_results)
            loader.save_results(results_df, "backtesting", "backtest_results.csv")
            self.logger.info("Backtest results saved using DataLoader")

        # Save accuracy metrics
        if self.accuracy_metrics:
            metrics_df = pd.DataFrame(self.accuracy_metrics)
            loader.save_results(metrics_df, "backtesting", "accuracy_metrics.csv")
            self.logger.info("Accuracy metrics saved using DataLoader")

        # Save forecast comparisons
        if self.forecast_comparisons:
            comparison_df = pd.DataFrame(self.forecast_comparisons)
            loader.save_forecast_comparison(comparison_df)
            self.logger.info("Forecast comparisons saved using DataLoader")

            # Generate forecast visualization data
            self._generate_forecast_visualization_data(comparison_df)

        # Save optimized parameters
        self._save_optimized_parameters()

    def _generate_forecast_visualization_data(self, comparison_df: pd.DataFrame):
        """Generate forecast visualization data with historical and forecast information using FULLY VECTORIZED operations."""
        self.logger.info("Generating forecast visualization data using FULLY VECTORIZED operations")

        # OPTIMIZATION: Load and prepare demand data once
        demand_df = self.demand_data.copy()
        demand_df["date"] = pd.to_datetime(demand_df["date"]).dt.date

        # OPTIMIZATION: Group and aggregate data in batch operations
        grouped_data = (
            comparison_df.groupby(
                ["product_id", "location_id", "forecast_method", "analysis_date"]
            )
            .agg(
                {
                    "risk_period": "first",
                    "demand_frequency": "first",
                    "first_date_used": "first",
                }
            )
            .reset_index()
        )

        # OPTIMIZATION: Merge with product master data in one operation
        product_master_df = self.product_master_data.copy()
        grouped_data = grouped_data.merge(
            product_master_df[["product_id", "location_id", "forecast_window_length", "risk_period"]].rename(
                columns={"risk_period": "pm_risk_period"}
            ),
            on=["product_id", "location_id"],
            how="left"
        )
        
        # Calculate window_length vectorized
        grouped_data["window_length"] = (
            grouped_data["forecast_window_length"].fillna(25) * 
            grouped_data["pm_risk_period"].fillna(1)
        )

        # OPTIMIZATION: Pre-aggregate forecast data by groupings to avoid repeated filtering
        forecast_aggregated = (
            comparison_df.groupby(["product_id", "location_id", "forecast_method", "analysis_date"])
            .apply(lambda x: {
                "risk_period_start": x.sort_values("step")["risk_period_start"].tolist(),
                "actual_demand": x.sort_values("step")["actual_demand"].tolist(),
                "forecast_demand": x.sort_values("step")["forecast_demand"].tolist(),
                "forecast_error": x.sort_values("step")["forecast_error"].tolist(),
            })
            .reset_index(name="forecast_data")
        )
        
        # Merge forecast data with grouped data
        grouped_data = grouped_data.merge(forecast_aggregated, on=["product_id", "location_id", "forecast_method", "analysis_date"])

        # OPTIMIZATION: FULLY VECTORIZED HISTORICAL CALCULATIONS
        self.logger.info(f"Processing {len(grouped_data)} visualization records with FULLY VECTORIZED operations")
        
        # Add progress tracking for historical bucket dates
        from tqdm import tqdm
        grouped_data["historical_bucket_start_dates"] = grouped_data.apply(
            lambda row: self._generate_historical_bucket_dates(
                row["analysis_date"], 
                row["first_date_used"], 
                row["risk_period"], 
                row["demand_frequency"]
            ), axis=1
        )
        
        # Convert grouped_data to list of dictionaries for parallel processing
        records = grouped_data.to_dict('records')
        
        # Process in parallel with progress bar using ThreadPoolExecutor (better for I/O bound tasks)
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import multiprocessing as mp
        
        max_workers = min(mp.cpu_count(), len(records), 8)  # Limit to 8 workers max
        
        historical_demands = [None] * len(records)
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all tasks
            future_to_index = {
                executor.submit(
                    self._calculate_historical_demands_parallel_wrapper,
                    record,
                    demand_df
                ): i for i, record in enumerate(records)
            }
            
            # Collect results with progress bar
            with tqdm(total=len(records), desc="Calculating Historical Demands for Visualisation", unit="records") as pbar:
                for future in as_completed(future_to_index):
                    index = future_to_index[future]
                    try:
                        historical_demands[index] = future.result()
                    except Exception as e:
                        self.logger.error(f"Error calculating historical demands for record {index}: {e}")
                        historical_demands[index] = []
                    pbar.update(1)
        
        # Assign results back to grouped_data
        grouped_data["historical_demands"] = historical_demands
        
        # OPTIMIZATION: Create visualization DataFrame directly from grouped_data
        visualization_df = grouped_data.copy()
        
        # OPTIMIZATION: Extract forecast data vectorized
        visualization_df["forecast_horizon_start_dates"] = visualization_df["forecast_data"].apply(lambda x: x["risk_period_start"])
        visualization_df["forecast_horizon_actual_demands"] = visualization_df["forecast_data"].apply(lambda x: x["actual_demand"])
        visualization_df["forecast_horizon_forecast_demands"] = visualization_df["forecast_data"].apply(lambda x: x["forecast_demand"])
        visualization_df["forecast_horizon_errors"] = visualization_df["forecast_data"].apply(lambda x: x["forecast_error"])
        
        # OPTIMIZATION: Drop the forecast_data column as it's no longer needed
        visualization_df = visualization_df.drop(columns=["forecast_data", "pm_risk_period", "forecast_window_length"])
        
        # OPTIMIZATION: Convert datetime objects and numpy types using vectorized operations
        # Convert historical bucket dates
        visualization_df["historical_bucket_start_dates"] = visualization_df["historical_bucket_start_dates"].apply(
            lambda x: [d.strftime("%Y-%m-%d") if isinstance(d, date) else str(d) for d in x]
        )
        
        # Convert forecast horizon dates
        visualization_df["forecast_horizon_start_dates"] = visualization_df["forecast_horizon_start_dates"].apply(
            lambda x: [d.strftime("%Y-%m-%d") if isinstance(d, date) else str(d) for d in x]
        )
        
        # Convert numeric lists to float lists
        numeric_columns = [
            "historical_demands",
            "forecast_horizon_actual_demands", 
            "forecast_horizon_forecast_demands",
            "forecast_horizon_errors"
        ]
        
        for col in numeric_columns:
            if col in visualization_df.columns:
                visualization_df[col] = visualization_df[col].apply(
                    lambda x: [float(val) for val in x]
                )

        # Save using DataLoader
        filename = self.config.loader.config['paths']['output_files']['forecast_visualization']
        self.config.loader.save_results(visualization_df, "backtesting", filename)
        self.logger.info("Forecast visualization data saved using DataLoader")

    def _generate_historical_bucket_dates(
        self,
        analysis_date: date,
        first_date_used: date,
        risk_period: int,
        demand_frequency: str,
    ) -> List[date]:
        """Generate historical bucket start dates going backwards from analysis date."""
        bucket_dates = []

        # Start one demand frequency before analysis date
        if demand_frequency == "d":
            start_date = analysis_date - timedelta(days=1)
        elif demand_frequency == "w":
            start_date = analysis_date - timedelta(weeks=1)
        elif demand_frequency == "m":
            # Approximate month as 30 days
            start_date = analysis_date - timedelta(days=30)
        else:
            start_date = analysis_date - timedelta(days=1)

        # Generate bucket start dates going backwards
        current_date = start_date
        while current_date >= first_date_used:
            bucket_dates.append(current_date)

            # Move backwards by risk period (converted to days)
            if demand_frequency == "d":
                current_date = current_date - timedelta(days=risk_period)
            elif demand_frequency == "w":
                current_date = current_date - timedelta(days=risk_period * 7)
            elif demand_frequency == "m":
                current_date = current_date - timedelta(days=risk_period * 30)
            else:
                current_date = current_date - timedelta(days=risk_period)

        # Reverse to get chronological order and exclude dates before first_date_used
        bucket_dates = [d for d in reversed(bucket_dates) if d >= first_date_used]
        return bucket_dates

    def _calculate_historical_demands(
        self,
        demand_df: pd.DataFrame,
        product_id: str,
        location_id: str,
        bucket_start_dates: List[date],
        risk_period: int,
        demand_frequency: str,
    ) -> List[float]:
        """Calculate historical demands for each bucket."""
        historical_demands = []

        for i, bucket_start in enumerate(bucket_start_dates):
            # Calculate bucket size in days based on demand frequency and risk period
            if demand_frequency == "d":
                bucket_size_days = risk_period
            elif demand_frequency == "w":
                bucket_size_days = risk_period * 7
            elif demand_frequency == "m":
                bucket_size_days = risk_period * 30
            else:
                bucket_size_days = risk_period

            # Calculate bucket end date
            bucket_end = bucket_start + timedelta(days=bucket_size_days - 1)

            # Filter demand data for this bucket
            bucket_demand = demand_df[
                (demand_df["product_id"] == product_id)
                & (demand_df["location_id"] == location_id)
                & (demand_df["date"] >= bucket_start)
                & (demand_df["date"] <= bucket_end)
            ]

            # Sum the demand for this bucket
            total_demand = bucket_demand["demand"].sum()
            historical_demands.append(total_demand)

        return historical_demands

    def _calculate_historical_demands_cached(
        self,
        product_data: pd.DataFrame,
        analysis_date: date,
        first_date_used: date,
        risk_period: int,
        demand_frequency: str,
    ) -> List[float]:
        """
        Calculate historical demands using cached product data for better performance.
        This method leverages pre-filtered data to avoid repeated filtering operations.
        """
        # Generate historical bucket start dates
        historical_bucket_start_dates = self._generate_historical_bucket_dates(
            analysis_date, first_date_used, risk_period, demand_frequency
        )
        
        historical_demands = []
        
        for bucket_start in historical_bucket_start_dates:
            # Calculate bucket size in days based on demand frequency and risk period
            if demand_frequency == "d":
                bucket_size_days = risk_period
            elif demand_frequency == "w":
                bucket_size_days = risk_period * 7
            elif demand_frequency == "m":
                bucket_size_days = risk_period * 30
            else:
                bucket_size_days = risk_period

            # Calculate bucket end date
            bucket_end = bucket_start + timedelta(days=bucket_size_days - 1)

            # Filter cached product data for this bucket - much faster than full dataset filtering
            bucket_demand = product_data[
                (product_data["date"] >= bucket_start)
                & (product_data["date"] <= bucket_end)
            ]

            # Sum the demand for this bucket
            total_demand = bucket_demand["demand"].sum()
            historical_demands.append(total_demand)

        return historical_demands

    def _calculate_historical_demands_vectorized(
        self,
        product_id: str,
        location_id: str,
        analysis_date: date,
        first_date_used: date,
        risk_period: int,
        demand_frequency: str,
        demand_df: pd.DataFrame
    ) -> List[float]:
        """
        Calculate historical demands using vectorized operations for better performance.
        This method optimizes the historical demand calculation by using cached data when available.
        """
        # OPTIMIZATION: Use cached product data if available
        product_key = f"{product_id}_{location_id}"
        if hasattr(self, 'product_data_cache') and product_key in self.product_data_cache:
            # Use cached data for faster calculations
            product_data = self.product_data_cache[product_key]
            
            # Generate historical bucket start dates
            historical_bucket_start_dates = self._generate_historical_bucket_dates(
                analysis_date, first_date_used, risk_period, demand_frequency
            )
            
            # Calculate historical demands using cached data
            return self._calculate_historical_demands_cached(
                product_data,
                analysis_date,
                first_date_used,
                risk_period,
                demand_frequency,
            )
        else:
            # Fallback to original method with full demand_df
            historical_bucket_start_dates = self._generate_historical_bucket_dates(
                analysis_date, first_date_used, risk_period, demand_frequency
            )
            return self._calculate_historical_demands(
                demand_df,
                product_id,
                location_id,
                historical_bucket_start_dates,
                risk_period,
                demand_frequency,
            )

    def _calculate_historical_demands_parallel_wrapper(self, record: Dict, demand_df: pd.DataFrame) -> List[float]:
        """
        Wrapper method for parallel processing of historical demand calculations.
        This method is designed to be called from ProcessPoolExecutor.
        """
        try:
            return self._calculate_historical_demands_vectorized(
                record["product_id"],
                record["location_id"], 
                record["analysis_date"],
                record["first_date_used"],
                record["risk_period"],
                record["demand_frequency"],
                demand_df
            )
        except Exception as e:
            # Return empty list on error to avoid breaking the parallel processing
            return []

    def _save_optimized_parameters(self):
        """Save optimized parameters to CSV file."""
        if not self.optimized_parameters_cache:
            self.logger.info("No optimized parameters to save")
            return

        try:
            # Initialize DataLoader
            from data.loader import DataLoader
            loader = DataLoader()

            # Convert optimized parameters cache to DataFrame
            optimized_params_list = []
            
            for product_key, parameters in self.optimized_parameters_cache.items():
                # Parse product key to extract components
                parts = product_key.split('_')
                if len(parts) >= 3:
                    # Handle cases where product_id or location_id might contain underscores
                    # Assume the last part is forecast_method, second to last is location_id
                    forecast_method = parts[-1]
                    location_id = parts[-2]
                    product_id = '_'.join(parts[:-2])  # Everything before the last two parts
                    
                    # Create row with product info and parameters
                    row = {
                        'product_id': product_id,
                        'location_id': location_id,
                        'forecast_method': forecast_method,
                        'optimization_timestamp': datetime.now().isoformat()
                    }
                    
                    # Add all parameters to the row
                    for param_name, param_value in parameters.items():
                        # Convert parameter values to string for CSV storage
                        if isinstance(param_value, (list, dict)):
                            row[param_name] = str(param_value)
                        else:
                            row[param_name] = param_value
                    
                    optimized_params_list.append(row)
                else:
                    self.logger.warning(f"Invalid product key format: {product_key}")

            if optimized_params_list:
                # Create DataFrame and save
                optimized_params_df = pd.DataFrame(optimized_params_list)
                
                # Save using DataLoader
                loader.save_results(optimized_params_df, "backtesting", "optimized_parameters.csv")
                
                self.logger.info(f"âœ… Optimized parameters saved for {len(optimized_params_df)} product-location-method combinations")
                self.logger.info(f"ðŸ“ File saved as: optimized_parameters.csv")
                
                # Log summary of parameters
                if 'forecast_method' in optimized_params_df.columns:
                    method_counts = optimized_params_df['forecast_method'].value_counts()
                    self.logger.info("ðŸ“Š Parameter optimization summary by method:")
                    for method, count in method_counts.items():
                        self.logger.info(f"   {method}: {count} products")
            else:
                self.logger.warning("No valid optimized parameters found to save")

        except Exception as e:
            self.logger.error(f"Error saving optimized parameters: {e}", exc_info=True)

    def _generate_summary(self, total_time: float) -> Dict[str, Any]:
        """Generate backtesting summary."""
        summary = {
            "backtesting_type": "unified",
            "execution_time": total_time,
            "analysis_period": {
                "start": self.config.analysis_start_date,
                "end": self.config.analysis_end_date,
                "dates": len(self.config.get_analysis_dates()),
            },
            "data_summary": {
                "total_demand_records": len(self.demand_data),
                "original_product_records": len(self.product_master_data),
                "expanded_product_records": len(self.expanded_product_master_data),
                "outlier_records": (
                    len(self.outlier_data) if self.outlier_data is not None else 0
                ),
            },
            "results_summary": {
                "total_forecasts": len(self.backtest_results),
                "total_comparisons": len(self.forecast_comparisons),
                "products_optimized": len(self.optimized_parameters_cache),
            },
            "product_groups": {
                method: len(products)
                for method, products in self.product_groups.items()
            },
            "configuration": {
                "demand_frequency": self.config.demand_frequency,
                "max_workers": self.config.max_workers,
                "batch_size": self.config.batch_size,
            },
        }

        return summary


def run_unified_backtest(config: BacktestConfig) -> Dict[str, Any]:
    """Run unified backtesting with the given configuration."""
    backtester = UnifiedBacktester(config)
    return backtester.run()
